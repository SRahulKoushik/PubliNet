# HiQA: A Hierarchical Contextual Augmentation RAG for Multi-Documents QA

Xinyue Chen, Pengyu Gao, Jiangjiang Song, Xiaoyang Tan.

#### Abstract

Retrieval-augmented generation (RAG) has rapidly advanced the language model field, particularly in question-answering (QA) systems. By integrating external documents during the response generation phase, RAG significantly enhances the accuracy and reliability of language models. This method elevates the quality of responses and reduces the frequency of hallucinations, where the model generates incorrect or misleading information. However, these methods exhibit limited retrieval accuracy when faced with numerous indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced multi-document questionanswering (MDQA) framework that integrates cascading metadata into content and a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-ofthe-art performance in multi-document environments.

### Introduction

Large Language Models (LLMs) have gained widespread popularity and accessibility, resulting in impressive applications across various domains (Vaswani et al. 2017; Brown et al. 2020; Bommasani et al. 2022; Chowdhery et al. 2023; Xiong et al. 2021; OpenAI 2023). One such domain is document question-answering(QA) (Saad-Falcon et al. 2023; Lala et al. 2023; Rajabzadeh et al. 2023), driven by the ´ significant demand for document reading among people or question-answering system in open-domain. Using only LLMs for QA still presents challenges such as hallucination issues (Ji et al. 2023), timeliness concerns, and insufficient pretrained problems. Retrieval-Augmented Generation (RAG) is a promising solution to these problems (Lewis et al. 2020). Nonetheless, standard RAG-based document QA systems predominantly represent documents as unstructured text chunks. This approach encounters limitations as document sizes increase, especially when dealing with documents that have similar and complex content or structures. Compared to single-document question-answering, multidocument question-answering poses more significant challenges as it requires considering the relationships and distinctions between documents. As the number of documents increases, the accuracy of responses continuously declines; we identify this issue as "RAG degradation in indistinguishable multi-documents." As results shown in Figure 2. When Compare the **features** of **TPS272C45** and **TPS281C100x**, what is the difference?

**Query** [Doc Title] **TPS272C45** High-Side Switch [Chapter Title] Product **Features [Chapter content]** …… - Wide DC voltage range: 6V to 36V - Low static current power supply: 0.5mA - Package: 24-pin QFN (5mm × 4mm) …… … Middle contents … … Rest contents … [Doc Title] **TPS281C100x** High-Side Switch [Chapter Title] Product **Features [Chapter content]** …… - Wide voltage range: 6 V to 60 V - Low quiescent current of < 1.5 mA - DNT (WSON, 12) 4.00 mm × 4.00 mm …… … Middle contents … … Rest contents … The detailed content in the **[Chapter content]** may not be accurately matched to the Query. **[Title]: TPS272C45/ TPS281C100x** + **[Chapter Title]: Features** can be matched to the Query **Other Docs RAG After Text Enhancement**

![](_page_0_Figure_7.jpeg)

numerous documents have similar content and outcomes, direct retrieval does not always produce accurate and relevant results. Therefore, data augmentation (Saad-Falcon et al. 2023; Zhao et al. 2024; Huang et al. 2023; Lu et al. 2022) serves as a potential solution to enhance the original documents for improved responses, as illustrated in Figure 1.

Our intuitive idea is that the key to using RAG in Documents QA is matching the "critical chunk" of knowledge to answer the query (Q) within the documents. This is analogous to archery, where the query acts as the arrow, and we need to ensure that the critical knowledge is within the target area. Therefore, by incorporating "definitional" text into the chunks, we can adjust their distribution, making it easier for the query embedding to hit the critical chunk.

The retrieval challenges posed by similar documents have not been fully addressed in existing RAG-based systems. Our practical experience has highlighted a particular multidocument question-answering scenario that standard RAG models struggle with. This involves large-scale document collections with approximately similar structures and content, such as product manuals from Texas Instruments, various iPhone models, company financial reports, and medical diagnosis and treatment manuals.

Current efforts often focus on considering the relationships between documents (Lu et al. 2019; Wang et al. 2023; Pereira et al. 2023; Caciularu et al. 2023), leveraging the reasoning abilities of LLMs to integrate information across different documents. PDFTriage (Saad-Falcon et al. 2023) addresses multi-documents QA tasks for structured documents by extracting the structural elements of documents and transforming them into retrievable metadata. The use of metadata by PDFTriage can be characterized as a hard partitioning technique. This strategy equals pruning and selection of subsets before information retrieval. Such measures are implemented to refine retrieval precision by diminishing the sizes of the segments. However, in scenarios involving complex tasks such as cross-document searches, useful knowledge risk being lost before retrieval in hard partitioning methods.

In scenario with large document collections, the content within the same chapters across different documents varies only slightly, making it difficult for RAG-based questionanswering systems to distinguish between them. Additionally, user queries often reference meta-information, like the "path of the title tree," exemplified by questions such as "features of the A100 GPU?" These queries necessitate navigation to specific chapters like 'Features,' posing a significant challenge in accurately retrieving and generating responses from large, structurally similar document sets.

To address this challenge, we propose HiQA(Hierarchical Contextual Augmentation RAG for Multi-Documents QA), incorporating a novel document parsing and conversion methodology. This approach includes a metadata-based augmentation strategy to enhance chunk distinguishability as well as a sophisticated Multi-Route retrieval mechanism. Tailored specifically for multi-document environments, our method aims to boost the precision and relevance of knowledge retrieval, overcoming the inherent limitations of traditional vector-based retrieval systems. This enhancement significantly improves the performance of RAG-based systems in managing the intricate demands of multi-document question answering (MDQA). The framework of our approach is depicted in Figure 3. To the best of our knowledge, this method of text augmentation has been rarely studied in the current literature. We have also made the codebase as well as datasets of our project publicly accessible to encourage further research and foster collaboration within the community.

The principal contributions of this paper are as follows:

- We identify a practically significant challenge, the indistinguishable multi-documents problem, which standard RAG struggles to address.
- We proposed our *HiQA* framework that utilizes cascading metadata, which is an effective solution to the indistinguishable multi-documents problem and facet seldom addressed in previous research.
- We release a benchmark, *MasQA*, comprising various types of multi-document corpora and multiple-question patterns, to facilitate research and assessment in MDQA scenario.

![](_page_1_Figure_8.jpeg)

Figure 2: Experimental validation of performance degradation in multi-documents QA scenario. Testing with 88 documents, each containing one of 88 questions. Using a vanilla RAG and GPT-4 setup (chunk size=400, top-k=5). Only one incorrect answer when querying each question on a single document. However, querying all 88 documents together leads to 30 incorrect answers, demonstrating significant degradation as the number of documents increases.

### Related Work

#### Retrieval-Augmented Generation

Retrieval-Augmented Generation(RAG) has demonstrated outstanding performance in knowledge-intensive NLP tasks, including open-domain question-answering, abstract question generation, and fact verification (Lewis et al. 2020). It has been effectively applied to clinical medicine data (Soong et al. 2023) and biomedical data (Zakka et al. 2023). In RAG, documents are typically segmented into chunks and converted into embeddings for storage, which are then used for subsequent retrieval. Therefore, the performance of the embedding model significantly impacts the effectiveness of RAG. Commonly used embedding models include BGE (Xiao et al. 2023), M3E1, OpenAI's text-ada-002, and others.

### Document QA

LlamaIndex (Smith and Doe 2023) employs a novel indexing strategy that integrates deep learning models with traditional information retrieval systems to create a dynamic query-responsive index. This system is particularly effective in environments where the information needs are diverse, and the document collections are large and complex. LangChain (Brown and Green 2023), on the other hand, combines language models with blockchain technology to ensure the integrity and traceability of the sources used in answering queries. By leveraging block-chain, LangChain creates a transparent and verifiable record of the data retrieval and processing steps, enhancing trust in the generated answers, especially in fields requiring high data fidelity, such as legal and financial documents.

### Multi-Document QA

Compared to single-document question-answering, multidocument question-answering necessitates considering the

![](_page_2_Figure_0.jpeg)

Figure 3: HiQA Framework. Illustration of the proposed framework. Initially, each document undergoes processing by a Markdown Formatter, transforming it into [chapter metadata: chapter content] pairs (termed segments) according to its inherent chapter structure, and is then stored in Markdown format. Subsequently, we extract the segment's hierarchy, and metadata is cascaded into each chapter, to build our database. Finally, we apply a Multi-Route retrieval method to enhance the RAG. Since hierarchical augmentation precedes retrieval, it offers a scalable solution to integrate with various embedding or retrieval methodologies seamlessly.

relationships and distinctions between documents, making it more challenging. (Lu et al. 2019; Wang et al. 2023) employs knowledge graphs to model relationships between documents and paragraphs. (Caciularu et al. 2023) models multi-document scenarios through pre-training. In contrast to these works that mainly focus on the issue of connections between multiple documents, this paper primarily investigates the retrieval problem for multi-documents with similar structures.

#### Data Augmentation for RAG

Data augmentation plays a pivotal role in enhancing the performance of RAG systems, particularly in the context of multi-document question answering (Zhao et al. 2024). The Make-An-Audio (Huang et al. 2023) system utilizes audiotext retrieval alongside caption generation for audio files that lack linguistic content. Similarly, LESS (Xia et al. 2024) selects optimal datasets for specific downstream tasks by analyzing gradient information from model training processes. Moreover, ReACC (Lu et al. 2022) introduces unique data augmentation techniques, such as renaming and dead code insertion, during the pre-training phase of code retrieval models.

### Methodology

Our proposed HiQA system is composed of three components: Markdown Formatter (MF), Hierarchical Contextual Augmentor (HCA), and Multi-Route Retriever (MRR). The MF module processes the source document, converting it into a markdown file, a sequence of segments. Rather than dividing the document into fixed-size chunks, each segment corresponds to a natural chapter, comprising both chapter metadata and content. HCA module extracts the hierarchical metadata from the markdown and combines it, forming cascading metadata, thereby augmenting the information of each segment. The MRR module employs a Multi-Route retrieval approach to find the most suitable segments, which are then provided as context inputs to the Language Model.

### Markdown Formatter

Given the necessity of acquiring hierarchical structural information for our proposed method, the source document must undergo structural parsing. Markdown is thus chosen for its excellent structured document formatting capabilities. Consequently, we introduce the Markdown Formatter to convert the source document into a Markdown document enriched with structural metadata.

Markdown Formatter employs an LLM for document parsing. The decision to use an LLM is driven by its ability to handle coherent contexts across pages by leveraging historical information, as well as its capacity for semantic comprehension and punctuation usage. These capabilities enable precise chapter segmentation and effective table data recovery, capitalizing on the LLM's advanced semantic understanding capabilities (Zhao et al. 2023).

Specifically, LLM Mc takes a PDF document DI as input and outputs a markdown-formatted document DM. The language model Mc is usually context-restricted, or there are

![](_page_3_Figure_0.jpeg)

Figure 4: Markdown Formatter. This demonstrates the extraction of chapter metadata and associated content from a long document and ensures alignment under sliding window processing.

problems with precision loss, forgetting, instruction weakening, hallucination, etc. when entering a long context. To ensure the structure of the output content is coherent, accurate, and consistent with the original document, we employ a sliding window technique with a window size of W, a step size of W, and additional padding of K. A document of length N requires T = ⌈N/W⌉ time steps for processing. The input and output documents are represented as sequences DI = {D (1) I , D(2) I , ..., D(T) I } and DM = {D (1) M , D(2) M , ..., D(T) M } respectively. The model's processing is formalized as:

$$D_{M}^{(t)}={\mathcal{M}}_{c}(D_{I}^{(t)},D_{I}^{(t-1)},D_{M}^{(t-1)})\qquad\qquad(1)$$

We use input and responses from the last round (D (t−1) I , D(t−1) M ) to calibrate the current round as there are overlapping. Figure 4 illustrates this step.

In addition, to ensure high-quality document processing, we provide meticulously designed instructions for the language model. The core ideas include:

- Treating every chapter in the document, regardless of its level, as a first-level heading in Markdown with a numerical identifier. We regard each chapter as a knowledge segment rather than a fixed-size chunk.
- Setting a correct chapter number, followed by the chapter title.
- Generating tables by Markdown syntax and recording the table titles.

Consequently, the resultant document DM comprises a series of segments, delineated as the sequence DM = {D (1) M , D(2) M , ..., D(S) M }. It is pertinent to note that S ̸= T, where S represents the count of segments, while T, contingent upon the dimensions of the processing window and the document's length, determines the number of segmented text blocks.

In Appendices A.2 to A.5, we illustrate specialized processes for handling tables and images, enabling the extraction of metadata from these elements and facilitating responses based on image content.

#### Hierarchical Contextual Augmentor

The Hierarchical Contextual Augmentor (HCA) module is employed to extract structure metadata from markdown files. It processes structure metadata and contextual information differently based on segment types, namely text, table, or image, forming corresponding cascading metadata for enhanced segments. The augmented segments are then transformed into embedding vectors using an embedding model and stored in a vector database.

Text Augmentation Upon processing the input document DI into a sequence of chapters DP with |DP | = S, each chapter is enriched with its metadata, including titles and numbering. We introduce a cascading metadata construction approach to address the inaccuracy in knowledge recall for extended, multiple, or similar documents. The document's hierarchical structure, akin to a tree with the document title as its root and chapters as nodes, is utilized. Our cascading metadata augmentation algorithm employs a depth-first search to traverse this chapter tree, concatenating and passing down metadata.

#### Multi-Route Retriever

In this section, we present our Multi-Route Retrieval approach for QA tasks that integrates various techniques to enhance the precision of knowledge retrieval from extensive document corpora. Specifically, we have implemented retrieval using the following three methods:

- Vector similarity matching
- Elastic search with BM25 (Elastic 2024)
- Keyword matching. We employ the Critical Named Entity Detection (CNED) method, utilizing a pre-trained named entity detection model to extract these pivotal keywords from documents as well as queries.

Compensating for Vector Similarity Limitations The performance of vanilla RAG systems declines with similar documents because these systems rely heavily on vector similarity. When the documents to be retrieved have consistent formatting and closely related content, they exhibit Algorithm 1: PDF2Markdown Formatting

Input: PDF document DI

Parameter: Window size W, Padding K, Language model Mc

Output: Markdown format document DM that contains chapter title, index, and level.

- 1: Calculate total iterations T = ⌈N/W⌉, where N is the number of words in DI .
- 2: for t = 1 to T do
- 3: Clip input segment D (t) I of length W + 2 × K with overlap.

* [4] Generate output segment $D_{M}^{(t)}$ by ${\cal M}_{c}(D_{I}^{(t)},D_{I}^{(t-1)},D_{M}^{(t-1)})$.  
  

- 5: Record (D (t−1) I , D(t−1) M ) from the current iteration to calibrate the next round.
6: end for

- 7: Compile DM from T segments into S chapters.
- 8: return DM

high semantic similarity, making them difficult to distinguish in the vector space. E.g., Differentiating similar documents such as "iPhone10" and "iPhone15" can be challenging with traditional RAG systems due to their heavy reliance on vector similarity, which often fails to distinguish between closely related content within extensive document collections. This issue is particularly problematic in scenarios where the documents have high semantic similarity despite minor differences like production date or battery capacity, leading to frequent retrieval errors. To address these issues, we implement a Lucene Index that focuses on frequency-based token appearance to improve retrieval, overcoming the limitations of vector similarity that neglects full etoken occurrences. Additionally, we enhance retrieval accuracy by leveraging the named entity recognition and human expert-set keywords to assign additional weight to relevant chunks, helping to refine search engine scores and effectively distinguish between indistinguishable documents. This approach not only compensates for the limitations of vector-based methods but also incorporates human querying preferences into the retrieval process, ensuring more precise answers.

These three methods gradually weaken in retrieving semantic-level information and enhance in retrieving character-level information. Their capabilities complement each other, and therefore, they are combined for use. After obtaining three sets of rankings, we perform re-ranking based on the formula:

$$\text{score}=\alpha\cdot\text{score}_{v}+(1-\alpha)\cdot\text{score}_{r}+\beta\cdot\log(1+|C|)\tag{2}$$

where α and β are hyperparameters that balance the contribution of vector similarity and information retrieval scores, respectively, and |C| represents the number of critical keywords matched. This scoring system is designed to be adaptive, allowing for fine-tuning based on the specific requirements of the query and the document set. The top-k knowledge segments, as determined by the final score, are Algorithm 2: Hierarchical Contextual Augmenting

Input: Document DM with S sections, each section D (i) M comprising Level, Title, and Content

Output: Enhanced document D′ M with cascading metadata

- 1: Initialize hierarchy ← []
- 2: Initialize D′ M ← []
- 3: Split document into lines: lines ← Split DM into lines
- 4: for each line in lines do
- 5: if line.startswith("#") then
- 6: Append current section into D′ M
- 7: Extract hierarchy level and update hierarchy
- 8: Append hierarchy metadata to the current section.
- 9: else
- 10: Append line to current section.

11: end if

- 12: end for
13: return D′ M

then presented to the LLM model to generate a coherent and contextually relevant answer.

We adjust hyperparameters α and β to optimize retrieval across diverse document collections based on empirical observations. In well-structured collections with clear hierarchies, we increase α to enhance the use of augmented metainformation, while in less structured settings, we rely more on word frequency. The keyword bonus weight β is tailored according to the ratio of document count (|D|) to average page count, increasing β when this ratio is large to emphasize the significance of keywords, thereby fine-tuning our retrieval system to adapt effectively to the specific dynamics of each document collection.

## Dataset

## Metric for RAG of MDQA

We introduce the Log-Rank Index, a novel evaluation metric designed to measure the effectiveness of the RAG algorithm's document ranking. Unlike existing methods such as RAGAS (Es et al. 2023), our metric is specifically developed to measure the RAG algorithm and overcome limitations in large document corpora.

Existing methods like RAGAS heavily depend on LLMs for generating questions and answers, which can introduce additional noise and hallucinations due to the reliance on the relevance of question-answer pairs. This reliance often results in the LLMs' performance overshadowing the actual quality of the RAG process. These methods also focus on top-k results, which offer a limited view of effectiveness across the entire document corpus. For instance, evaluations based on top-k context precision are adequate for shorter documents but become inadequate when the target knowledge fails to achieve a top-k ranking, leading to a zero score in large document corpora. Conversely, traditional metrics such as precision@K (Schutze, Manning, and Raghavan ¨ 2008), MRR (Voorhees et al. 1999), and nDCG (Jarvelin ¨

and Kekal¨ ainen 2002) face their limitations. The log-rank ¨ index, a particular case of precision@N where N is the size of chunks, uses logarithmic smoothing to be highly sensitive to top rankings. This sensitivity becomes crucial in extensive, similar document collections where it is challenging to ensure relevant results appear within the top K, thus addressing the issue of sparse metrics. Unlike MRR, which focuses on ranking the first relevant document and is unsuitable for multi-document question-answering (MDQA) scenarios that require retrieving multiple chunks from multiple documents, the log-rank index also considers the relevance grade of documents. It uses logarithmic weighting to reflect the importance of different rankings, better capturing the nonlinear changes in information retrieval compared to nDCG's linear or exponential weighting.

Our proposed metric utilizes a nonlinear logarithmic ranking function, which is more sensitive in the higher-ranking region and thereby addresses the shortcomings of linear scoring methods (See Appendix A.6). The utilization of ranking as a metric ensures consistent and reliable assessment. Consequently, this approach offers a detailed and critical analysis of the RAG algorithm's efficacy. Finally, we believe this new metric is denser, more sensitive to top rankings, and smoother, making it an ideal machine-learning target.

Dataset and Definitions We keep similar questioncontext-answer triples in our evaluation (Es et al. 2023). We denote the dataset I = {(qi , ci , D)} K i=1, where K represents the number of samples, D is a document corpus consisting of N document segments, and di is a subset of D containing the indices of document segments to answer i th query qi . Let r denote the RAG algorithm being evaluated, and oi = r(qi , D) represent the array of rank for document segments which is ranked by r in response to qi .

Score Calculation The score for each query qi is computed based on the ranks of the relevant segments in oi . For a given query qi , if multiple document segments are relevant, the score for that query is the average of the scores for each relevant segments. The score for each segments is calculated using an inverted logarithmic scoring function, The scoring function is defined as:

$$S(r_{i})=1-\frac{\log(1+\gamma(r_{i}-1))}{\log(1+\gamma(N-1))}\tag{3}$$

where ri is the position of the i th segment in the ranked list oi , N is the total number of documents in D, and γ is constant parameter to control shape of the curve. Increasing γ leads to the curve dropping faster at high rankings.

#### The MasQA Dataset

To assess the proposed framework, we introduce the MasQA dataset. As existing datasets fail to capture the challenges posed by extensive document libraries and the abundance of similar documents, a gap that MasQA aims to bridge, highlighting the ability and potential applications of extracting information for QA from large document corpora.

Dataset Composition and Construction The MasQA dataset includes five distinct subsets, each specifically designed to represent different document scenarios. This variety ensures a thorough evaluation of RAG performance across diverse contexts and demonstrates its potential for real-world applications.

- Technical Manuals from Texas Instruments This subset includes 18 PDF files, each approximately 90 pages, featuring a mix of images, text, and tables in multiple languages.
- Technical Manuals from Chipanalog It consists of 88 PDF files, around 20 pages each, presented in a twocolumn format, enriched with images, text, and tables.
- A College Textbook A comprehensive 660-page book encompassing images, text, formulas, and tables.
- Public Financial Reports Listed Companies This consists of 8 reports for 2023, each report spans roughly 200 pages, mainly including text and tables.
- Official Medical Guides for Liver We collect 116 official guides for liver diseases.

Question Bank For each subset, we crafted a question bank comprising question-answer-context triples. To show the application prospect of the proposed method, the questions are designed to mimic inquiries by engineers and analysts, covering various dimensions:

- Single and Multiple Choice Questions Evaluating the dataset's capability to handle straightforward selectionbased questions. demo1
- Descriptive Questions Testing the ability to provide detailed explanations based on specific criteria.
- Comparative Analysis Involving multiple document segments for comparing several entities.
- Table Questions Assessing one or more tables extraction.
- Across documents Testing the ability to retrieve more than one document segment from multi-documents.
- Calculation Testing the ability to gather information related to the questions and complete calculation problems.

Each question is annotated with correct answers and corresponding document segments. We will employ the Log-Rank Index for RAG metrics and assess the final answer quality to evaluate our methodology's efficacy in handling large-scale document bases and diverse document types. Example questions are shown in Appendix B.4.

Finally, as illustrated in Appendix B.3, Our dataset's substantial size and practical utility accurately reflect the challenges faced in QA over large-scale document bases. These characteristics underscore the relevance and applicability of our approach in real-world scenarios.

### Experiment

In this section, we conduct a series of experiments. We validate the performance of HiQA by comparing state-of-the-art methods with the MasQA dataset. Subsequently, we employ

| Method |  | Texas Instruments | Chipanalog |  |  | Financial Report | Textbook |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | Accuracy | Adequacy | Accuracy | Adequacy | Accuracy | Adequacy | Accuracy | Adequacy |
| LlamaIndex | 0.674 | 4.87 | 0.51 | 3.79 | 0.605 | 4.89 | 0.158 | 4.21 |
| ChatPDF | 0.587 | 4.09 | 0.565 | 4.04 | 0.658 | 4.58 | 0.316 | 4 |
| GPT4-Doc | 0.913 | 3.61 | 0.583 | 4.04 | 0.684 | 4.68 | 0.263 | 4.26 |
| GPT3.5 | 0.536 | 3.23 | 0.497 | 3.55 | 0.497 | 4.42 | 0.177 | 3.61 |
| HiQA | 0.957 | 4.96 | 0.833 | 5 | 0.684 | 4.74 | 0.447 | 4.42 |

Table 1: QA evaluation

The metric adequacy is calculated by inversing the rank given by annotators. Higher adequacy means the answer is clear and informative. Results Highlight Challenges for Mainstream Document QA Methods with Multi-Documents.

| Log-rank Index | Mean | Max | Min | Std. | Accuracy |
| --- | --- | --- | --- | --- | --- |
| HCA | 0.98 | 1.0 | 0.90 | 0.02 | 0.95 |
| No Hierarchy | 0.97 | 1.0 | 0.78 | 0.06 | 0.79 |
| Original RAG | 0.95 | 1.0 | 0.61 | 0.09 | 0.75 |
| Fixed Chunk | 0.92 | 1.0 | 0.66 | 0.1 | 0.68 |
| Vec retrieval | - | - | - | - | 0.88 |

Table 2: Ablation Study Results. We also utilize the Logrank Index to evaluate how effectively our method improves the ranking of key knowledge chunks.

ablation studies to evaluate the effectiveness of each component. Finally, we aim to understand the influence of HCA on retrieval by visualizing the distribution of segments in the embedding space.

#### Query-Answering evaluation

We evaluated QA performance on the MasQA dataset using ChatGPT4, ChatGPT3.5, LlamaIndex (Smith and Doe 2023), ChatPDF, and HiQA. While the latest Chat-GPT4 shows decent performance in MDQA, HiQA demonstrates strong competitiveness, outperforming these advanced methods. As Table 1 illustrates, HiQA not only maintains high accuracy but also surpasses others in the rational organization of answers. Notably, HiQA excels in complex cross-document tasks, contributing significantly to its high accuracy. Additionally, our approach limits tokens to within 2k, in contrast to the average 4k used by other methods. By integrating HCA to elevate the ranking of target segments and utilizing chapters to minimize chunk noise, we effectively encompass necessary knowledge with fewer tokens.

#### Ablation Experiment

In the ablation study, we evaluate the contributions of various components within our framework by analyzing the QA performance of different variants. To specifically assess the impact of HCA on segment ranking, independent of LLMs effects, we utilize the Log-Rank index. Our study examines the ablation of HCA and Multi-Route Retrieval (MRR), resulting in five variants: 'HCA' represents our proposed framework; 'No Hierarchy' employs chapter metadata but without cascading; 'Original RAG' excludes chapter metadata; 'Vanilla Fixed Chunk' excludes our Markdown Formatter but retain MRR; and 'Vector Only Retrieval,' which replaces MRR. The results in Table 2 demonstrate that metainformation embedding and Multi-Route retrieval significantly contribute to the system's efficacy. Specifically, the Log-rank Index consistently improves with the augmentation of metadata, underscoring the significance of HCA in boosting retrieval precision. The results in QA performance also reflect this trend. Finally, the performance decline observed in the Vector Only Retrieval approach indicates inherent limitations in vector similarity methods. These shortcomings can be mitigated by integrating frequency-based retrieval techniques and keyword ranking strategies.

Finally, we conducted experiments to explore distribution in embedding space, and the results demonstrate that our approach effectively adjusts the distribution of multidocument chunks in the embedding space, making them more amenable to accurate retrieval. Further details can be found in Appendix C.

### Conclusion

In this paper, we introduce HiQA, a novel framework specifically designed to address the limitations of existing RAG in multi-document question-answering (MDQA) environments, particularly when dealing with indistinguishable multi-documents. HiQA incorporates a soft partitioning strategy that utilizes the structural metadata of documents for effective chunk splitting and embedding augmentation alongside a Multi-Route retrieval mechanism to enhance retrieval efficacy. Our extensive experiments validate the robustness and effectiveness of our approach, contributing to a deeper theoretical understanding of document segment distribution within the embedding space. Furthermore, we have developed and released the MasQA dataset, which offers substantial academic and practical value.

We also pioneer using cascading document structures for text enhancement during data processing, which integrates effectively with existing Retrieval-Augmented Generation (RAG) techniques. This innovation has drawn interest from leading RAG projects and has proven advantageous in healthcare and law. In these domains, where documents like medical guides and legal texts are inherently structured, our approach of augmenting cascading meta-information has demonstrated substantial soundness and utility, affirming its significance across various high-impact areas.

### References

Bommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.; Arora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut, A.; Brunskill, E.; et al. 2022. On the Opportunities and Risks of Foundation Models. arXiv:2108.07258.

Brown, A.; and Green, B. 2023. LangChain: Integrating Blockchain with Language Models for Transparent QA. In *Proceedings of the International Conference on Blockchain Technology*, 88–95.

Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. In *Advances in neural information processing systems*, 1877– 1901.

Caciularu, A.; Peters, M.; Goldberger, J.; Dagan, I.; and Cohan, A. 2023. Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, 1970–1989.

Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; et al. 2023. Palm: Scaling language modeling with pathways. *Journal of Machine Learning Research*, 24: 1–113.

Elastic. 2024. Accelerate time to insight with Elasticsearch and AI. https://www.elastic.co/.

Es, S.; James, J.; Espinosa-Anke, L.; and Schockaert, S. 2023. Ragas: Automated evaluation of retrieval augmented generation. *arXiv preprint arXiv:2309.15217*.

Huang, R.; Huang, J.; Yang, D.; Ren, Y.; Liu, L.; Li, M.; Ye, Z.; Liu, J.; Yin, X.; and Zhao, Z. 2023. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. In *International Conference on Machine Learning*, 13916–13932. PMLR.

Jarvelin, K.; and Kek ¨ al¨ ainen, J. 2002. Cumulated gain-based ¨ evaluation of IR techniques. *ACM Transactions on Information Systems (TOIS)*, 20(4): 422–446.

Ji, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y.; Ishii, E.; Bang, Y. J.; Madotto, A.; and Fung, P. 2023. Survey of hallucination in natural language generation. *ACM Computing Surveys*, 55: 1–38.

Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Kuttler, H.; Lewis, M.; Yih, W.-t.; Rockt ¨ aschel, ¨ T.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In *Advances in Neural Information Processing Systems*, 9459– 9474.

Lu, S.; Duan, N.; Han, H.; Guo, D.; Hwang, S.-w.; and Svyatkovskiy, A. 2022. Reacc: A retrieval-augmented code completion framework. *arXiv preprint arXiv:2203.07722*.

Lu, X.; Pramanik, S.; Roy, R. S.; Abujabal, A.; Wang, Y.; and Weikum, G. 2019. Answering Complex Questions by Joining Multi-Document Evidence with Quasi Knowledge Graphs. In *Proceedings of the 42nd International ACM SI-GIR Conference on Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019*, 105–114.

Lala, J.; O'Donoghue, O.; Shtedritski, A.; Cox, S.; Ro- ´ driques, S. G.; and White, A. D. 2023. PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. arXiv:2312.07559.

OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.

Pereira, J.; Fidalgo, R.; Lotufo, R.; and Nogueira, R. 2023. Visconde: Multi-document QA with GPT-3 and Neural Reranking. In *European Conference on Information Retrieval*, 534–543.

Rajabzadeh, H.; Wang, S.; Kwon, H. J.; and Liu, B. 2023. Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models. arXiv:2309.08922.

Saad-Falcon, J.; Barrow, J.; Siu, A.; Nenkova, A.; Yoon, D. S.; Rossi, R. A.; and Dernoncourt, F. 2023. PDF-Triage: Question Answering over Long, Structured Documents. arXiv:2309.08872.

Schutze, H.; Manning, C. D.; and Raghavan, P. 2008. ¨ *Introduction to information retrieval*, volume 39. Cambridge University Press Cambridge.

Smith, J.; and Doe, J. 2023. LlamaIndex: Dynamic Indexing for Document QA Systems. *Journal of AI Research*, 59: 101–120.

Soong, D.; Sridhar, S.; Si, H.; Wagner, J.-S.; Sa, A. C. C.; ´ Yu, C. Y.; Karagoz, K.; Guan, M.; Hamadeh, H.; and Higgs, B. W. 2023. Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model. arXiv:2305.17116.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In *Advances in neural information processing systems*, 5998–6008.

Voorhees, E. M.; et al. 1999. The trec-8 question answering track report. In *Trec*, volume 99, 77–82.

Wang, Y.; Lipka, N.; Rossi, R. A.; Siu, A.; Zhang, R.; and Derr, T. 2023. Knowledge Graph Prompting for Multi-Document Question Answering. arXiv:2308.11730.

Xia, M.; Malladi, S.; Gururangan, S.; Arora, S.; and Chen, D. 2024. Less: Selecting influential data for targeted instruction tuning. *arXiv preprint arXiv:2402.04333*.

Xiao, S.; Liu, Z.; Zhang, P.; and Muennighoff, N. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597.

Xiong, W.; Li, X. L.; Iyer, S.; Du, J.; Lewis, P.; Wang, W. Y.; Mehdad, Y.; Yih, W.-t.; Riedel, S.; Kiela, D.; et al. 2021. Answering complex open-domain questions with multi-hop dense retrieval. In *International Conference on Learning Representations*.

Zakka, C.; Chaurasia, A.; Shad, R.; Dalal, A. R.; Kim, J. L.; Moor, M.; Alexander, K.; Ashley, E.; Boyd, J.; Boyd, K.; et al. 2023. Almanac: Retrieval-augmented language models for clinical medicine. *Research Square*.

Zhao, B.; Ji, C.; Zhang, Y.; He, W.; Wang, Y.; Wang, Q.; Feng, R.; and Zhang, X. 2023. Large Language Models are Complex Table Parsers. *arXiv preprint arXiv:2312.11521*. Zhao, P.; Zhang, H.; Yu, Q.; Wang, Z.; Geng, Y.; Fu, F.; Yang, L.; Zhang, W.; and Cui, B. 2024. Retrieval-Augmented Generation for AI-Generated Content: A Survey. *arXiv preprint arXiv:2402.19473*.

## Appendix A

## A.1 Proposed Question-Answering System

In the proposed framework, the question-answering process is single-stepped. Initially, relevant knowledge is retrieved from the document base using RAG according to the query. Subsequently, this context, in conjunction with the question, is fed into the language model to generate a response. The time taken to return the first character of the answer ranges between 1 to 3 seconds. An example of the QA process is illustrated in Figure 5.

| Select a dataset: Texas Instruments |
| --- |
| Enter your question: Compare the features of AFEx8201 and TPS281C100x, what the difference? |
| Answer: |
| Based on the provided knowledge, here is a comparison of the AFEx8201 and the TPS281C100x: |
| AFEx8201 Features: |
| • It has a 12-bit SAR ADC that can multiplex measurements across different inputs including external and internal nodes for device |
| diagnostics. |
| • The ADC references its conversion from the VREFIO pin voltage and has an on -chip oscillator for timing signals. |
| • Features a monitoring system that can sense up to two external inputs as well as internal device signals. |
| • It also includes programmable out -of-range alarms and temperature sensor alarm functions. |
| ◦ When the die temperature exceeds 85 °C, a thermal warning alarm is triggered. |
| ◦ A thermal error alarm is set when the temperature surpasses 130 °C. |
| • Alarms can be configured to set an ALARM pin and indicated in the ALARM_STATUS register, with configurable actions through the ALARM _ACT |
| register. |
| TPS281C100x Features : |
| • The TPS281C100x is a single -channel smart high -side switch used in industrial control systems. |
| • It supports a wide operating range (up to 60V DC) and has a low RON to minimize power dissipation. |
| • The device integrates several protection features including thermal shutdown, output clamp, and current limit. |
| • Adjustable current limiting circuit to better handle inrush current for loads like large capacitive ones or to minimize overload current . |
| • It provides accurate load current sense which helps in diagnostics for predictive maintenance. |
| • The device is available in two package options with different sizes: |
| ◦ 14-pin, 5.00 mm x 4.40 mm HTSSOP |
| ◦ 12-pin, 4.00 mm x 4.00 mm WSON |
| Key Differences: |
| • The AFEx8201 focuses on ADC features and temperature monitoring with alarms, which is more oriented towards monitoring and sensin g |
| application. |
| • The TPS281C100x is designed for power switching with integrated protection features and load current diagnostics, which is more s uitable |
| for controlling power flow and managing output loads in industrial applications. |
| • They come in different package types and sizes, which may influence the choice depending on the space and layout constraints in th e |
| application they are designed for. |

Figure 5: An Example Query-Answering on Texas Instruments Dataset

## A.2 Hierarchical Metadata Augmentation

We inventively use cascading document structures for text enhancement during data processing as shown in 6, a technique that integrates seamlessly with existing RAG methods and has garnered attention from leading RAG projects. Moreover, in fields of high interest to LLMs like healthcare and law, where medical guides and legal documents exhibit structured formats, our cascading meta-information augmentation approach demonstrates strong soundness, offering significant utility.

![](_page_9_Figure_7.jpeg)

Figure 6: The cascading metadata embedding process. This step involves identifying the hierarchical metadata path of each segment from the root and subsequently augmenting this information into the segment.

## A.3 Image References in Responses

Our approach innovatively extends the MDQA framework by retrieving images from documents and incorporating these images in responses, as demonstrated in Figure 7.

For images, we developed a tool named PDFImageSearcher, which is open sourced, to extract bitmap and SVG vector images from documents, as well as an API to retrieve an image. This utilizes the text surrounding the image, the image title, and an optional visual language model to generate a descriptive file for each image. Each document DM will have an image collection DG = {I1(File1, Desc1), I2(File2, Desc2), ...}.

![](_page_10_Figure_1.jpeg)

Figure 7: An Example Query-Answering via Image Reference

### A.4 Table Augmentation

Traditional chunk-based RAG methods do not specifically address tables. Our experiments indicate difficulties in accurately recalling table information, largely because the numerical values in tables often behave as noise in semantic encoding. An example question is: "Does this phone have a 13,000 mah battery charge?". Actually, we need to match the battery rather than the number and use the retrieved number to fact-check. We posit that the semantic value of a table originates from its definition, including overall description, title, and row/column labels, as illustrated in Figure 8. Hence, in embedding tables, we focus solely on these semantic elements, treating tables akin to text knowledge.

![](_page_10_Figure_5.jpeg)

Figure 8: Embedding for Tables. Data fields are omitted to reduce noise during embedding. But if retrieved, these data fields are retained to provide context for LLMs

#### A.5 Image Augmentation

We utilize the wrapped context of image and can further leverage visual language generation models to create descriptive captions that encapsulate the salient features of the image. These captions are then embedded, allowing the model to answer with a figure. Image augmentation shown in Figure 9

![](_page_11_Figure_2.jpeg)

Figure 9: Embedding for Images. Applying a Visual-Language model to generate textual descriptions of the image semantics, which are then incorporated into the segment.

### Appendix B

#### B.1 Impact of Data Field Removal on Retrieval of Table

We examined the impact of removing data fields from tables during the embedding stage on the RAG method. As demonstrated in Table 3 and Figure 10, the removal of data fields increases the inner product of context and question in the embedding space and reduces their distance in this space.

![](_page_11_Figure_7.jpeg)

Figure 10: Embedding Shifting via Removing Data Fields of Table

| Method | Table | Data Fields Removal |
| --- | --- | --- |
| Inner product | 0.879 | 0.913 |

Table 3: Inner Product of Embedding between Question and Table Content

#### B.2 Evaluation Metrics

In the experimental section, we propose three metrics to evaluate the performance of the MDQA method: Accuracy, Adequacy, and the Log-rank Index. Accuracy refers to the correctness rate of answers, scored as 1 for correct, 0 for incorrect, and 0.5 for partially correct answers, applicable in short-answer and multiple-choice questions. Adequacy assesses whether answers possess clarity and informativeness. To compute this metric, annotators rank answers to the same question generated by various methods. Assuming there are K methods, if method i is ranked as ri , (i ∈ [1, K]), its Adequacy score is calculated as (K + 1) − ri , yielding scores in the range of 1 to K. Therefore, a higher rank corresponds to a higher Adequacy score, indicating better answer quality. The Log-rank Index evaluates the recall ability of the RAG method in context retrieval using a descending curve, as shown in Figure 11.

![](_page_12_Figure_4.jpeg)

Figure 11: Illustrate Log-rank Index with different γ

## B.3 Description of Datasets

We introduce five distinct datasets, Each dataset exhibits unique characteristics. a). Manuals of Texas Instructions This dataset consists of lengthy individual documents but has a lower count of documents. b). Manuals of Chipanalog This features shorter individual document lengths but encompasses a larger number of documents. Both the first and second datasets share similar document structures and content. c). Textbook about Analog Circuit Design This has extremely long document lengths with significant structural differences, enriched with formulas and images. d). Finacial Reports This dataset encompasses lengthy documents with identical formats and particularly similar content due to the same template, and containing extensive verbose tables and data, posing substantial challenges for analytical and comparative question-answering. e). Medical Guides for Liver This dataset comprises detailed documents on liver diseases, featuring structured sections that include symptoms, treatments, and prognoses. This dataset is enriched with medical terminology and often includes diagrams and patient care instructions, making it valuable for queries requiring in-depth medical knowledge and specific information retrieval.

Table 6 provides a detailed comparison of these datasets across multiple dimensions.

## B.4 Question Bank Example

In 4, we show example questions for each question type.

## Comparison of Different Models

See table 5, we show and compare the generated output from different models.

| Question Type | Examples |
| --- | --- |
| Single Choice | Chipanalog 485 series interface products can support the highest rate is () A: 10Mbps |
|  | B: 20Mbps C: 50Mbps D: 100Mbps AFE7906's features of each DDC channels have A |
|  | NCO (). A: 4 B: 8 C, 12 D: 16 |
| Multiple Choice | Applications of ADC12QJ1600-SP are (). A: Electronic warfare (Signals intelligence, |
|  | electronic intelligence) B: Satellite communications (SATCOM) C: Battery management |
|  | systems D: Circuit breakers |
| Judgement | Statement: In the noise analysis of a common-gate amplifier, the input end of the circuit |
|  | should be open when solving the equivalent input noise current source. True of False? |
| Descriptive | Please compare the production mode of the main business models of Zhejiang Huazheng |
|  | and Weijie Chuangxin. |
| Comparison | Single choice: Which of the following product of Chipanalog is an ultra-low-power digi |
|  | tal isolator? A: CA - IS3722HS B: CA - IS3742HW C: CS817x22HS D: CA - IS3841HW |
| Summary | What is the phase margin and what role does it play in system stability analysis? |
| Calculation | The parameters of the resistor-loaded common-source amplifier circuit are as follows: |
|  | ID = 100µA, RD = 25kΩ, MOS tube parameters are V T Hn = 1V , µnCOX = |
|  | −1 50µA/V 2 , λ = 0.1V , W/L = 50/2. [1.] The intrinsic transconductance Gm (RS = |
|  | 0Ω, RL = 0Ω) of the amplifier is calculated using the two-port model. [2.] In the case of |
|  | load resistance RL, in order to ensure that the transconductance iout/vs is greater than 10 |
|  | percent of the intrinsic transconductance, then the value range of RL? |
| Crossing Documents | Fill the blank: Aiwei's non-current asset disposal profit and loss amount in 2022 is ap |
|  | proximately () times that of Zhejiang Huazheng |
| Table Related | Single choice: The measurement range of OPT3004's characteristics is (). A: 0.001lux to |
|  | 10lux B: 1lux to 15lux C: 0.01lux to 83,000lux D: 20lux to 83lux |

Table 4: Example of Question Bank

## B.5 Incorporation of Large Language Models

In our framework, we mainly apply *gpt-4-1106-preview* from OpenAI in markdown formatting and question-answering. And utilize a pre-trained API *text-ada-002* from OpenAI for text embedding.

## B.6 Comparative Analysis of LLM types Input Length

In Table 7, we compare the accuracies of three models, GPT4, Moonshot, and ChatGLM-Pro, across different context lengths. This comparison aids in assessing the models' performance variations with token length changes.

# Appendix C

## C.1 Distribution Exploration in Documents

In this section, we demonstrate that HCA reshapes the distribution of document segments in the embedding space by strengthening the cohesion among segments and between questions and segments, raising a soft partition effect. Importantly, e.g., compared to fetch tools of PDFTriage, it enhances the retrieval accuracy of the RAG algorithm without any modifications to the algorithm itself, thereby avoiding the potential information loss associated with hard pruning.

We quantitatively analyze distribution movements via PCA and tSNE visualization on a two-dimensional plane. The first three experiments focused on observing the impact of HCA on the distribution of document segments. The last experiment more specifically examined the spatial distribution of vector representations for given question-context pairs (Target Segment) in the embedding space.

C.2 How does HCA improve cohesion within a single document? We selected a document and applied three embedding processing methods: with HCA, the Original Segment, and without HCA, then compared the three sets of embedding vectors using PCA and tSNE. The results depicted in Figure 13 (a) and (b) demonstrate that the implementation of HCA leads to a more compact distribution. These findings indicate that our approach can enhance the focus of the RAG algorithm on the target domain.

C.3 How does HCA improve cohesion among multi-documents We analyzed five documents from a dataset to compare their distributions with and without HCA. In a multi-document scenario, segments within each document naturally form a cluster. Thus, we can examine the distribution of these clusters. As illustrated in Figure 14, documents from the same dataset exhibit inherent similarities, leading to overlapping distributions and increasing retrieval complexity. However, data processed with HCA showed significant intra-cluster cohesion, effectively creating a soft partition of the documents which circumvents the information pruning associated with hard partitioning methods like Llamaindex.

| Question | Correct Answer | HiQA | GPT4 | Llamaindex | ChatPDF |
| --- | --- | --- | --- | --- | --- |
| Single choice: In the | D.Guangdong | Based on the | In the second | South Asia New | The specific data |
| main financial data of | Shengyi | information | quarter, the net | Materials | of the four |
| quarters in 2022, the net |  | provided, we can | profit attributable |  | companies of |
| profit after deducting |  | see that among | to shareholders of |  | Nanya New |
| non-recurring profit and |  | options A, C and | listed companies |  | Material, Weijie |
| loss attributable to |  | D, Guangdong | after deducting |  | Chuangxin, Aiwei |
| shareholders of listed |  | Sheng Yi had the | non-recurring |  | and Guangdong |
| companies in the second |  | highest net profit | gains and losses |  | Shengyi are not |
| quarter, which of the |  | after deducting | was the largest. |  | given in the |
| four companies: South |  | non-recurring | The answer is B: |  | provided |
| Asia New Material, |  | gains and losses in | Weijie Chuangxin. |  | knowledge. |
| Weijie Chuangxin, |  | the second quarter. |  |  | Therefore, I |
| AiWei and Guangdong |  | The answer is: D: |  |  | cannot answer this |
| Shengyi? A: South Asia |  | Guangdong |  |  | question |
| New Material B: Weijie |  | Shengyi |  |  | accurately. |
| Chuangxin C: Aiwei D: |  |  |  |  |  |
| Guangdong Shengyi |  |  |  |  |  |
| Fill the blanks: The | electrical | Electrical | Application | electrical and | The OPT3004's |
| OPT3004's Application | interface, optical | Interface, Optical | information for the | optical | Application |
| Information includes two | interface | Interface | OPT3004 includes |  | Information |
| categories of interface: |  |  | two interface |  | includes two |
| (), (). |  |  | categories: |  | categories of |
|  |  |  | electrical and |  | interface: |
|  |  |  | optical. |  | hardware and |
|  |  |  |  |  | software. |
| Multiple choice: Which | A: CA-IS3641x | A: CA-IS3641x. | The digital isolator | A: CA-IS3641x | According to the |
| of the following products |  | In the materials, | with power supply |  | information |
| is a digital isolator with |  | only the | is CA |  | provided, the |
| power supply () A: |  | CA-IS364x is | IS3092W/98W. |  | CA-IS364x is A |
| CA-IS3641x B: |  | described as a | This is an isolated |  | digital isolator |
| CA-IS3741x C: |  | product with | RS-485/RS-422 |  | with a power |
| CA-IS3841x D: |  | enhanced isolation | transceiver with |  | supply, so it is |
| CA-IS3020x |  | voltage resistance | integrated isolated |  | correct to select A: |
|  |  | and integrated | power supply with |  | CA-IS3641x. |
|  |  | DC-DC converter, | high |  |  |
|  |  | so it is a digital | electromagnetic |  |  |
|  |  | isolator with a | immunity. |  |  |
|  |  | power supply. | Therefore, in the |  |  |
|  |  |  | given option, B: |  |  |
|  |  |  | CA-IS3741x is a |  |  |
|  |  |  | digital isolator |  |  |
|  |  |  | with power supply. |  |  |

Table 5: Answers Across Different Methods

| Feature | TI | Chipanalog | Textbook | Financial Report | Medical Guide |
| --- | --- | --- | --- | --- | --- |
| Document Count | 18 | 88 | 1 | 8 | 116 |
| Average Page Count | 90 | 60 | 660 | 200 | 11 |
| Total Number of Chapters | 3196 | 1651 | 897 | 2172 | 5647 |
| Number of Problems | 25 | 50 | 20 | 20 | 20 |
| Annotated Context | Yes | No | No | No | No |
| Problem Difficulty | Low | Medium | Medium | High | High |

Table 6: Dataset Overview

| Model | Token Length | Accuracy (%) |
| --- | --- | --- |
| GPT4 | 2k | 44/50 (88%) |
| GPT4 | 8k | 48/50 (96%) |
| Moonshot | 2k | 42/50 (84%) |
| Moonshot | 8k | 45/50 (90%) |
| ChatGLM-Pro | 4k | 39/50 (78%) |

Table 7: Accuracy of Different Models with Various Token Lengths

![](_page_15_Figure_2.jpeg)

Figure 12: Statistical Information on the Scale of the Dataset: While typical RAG applications operate on datasets comprising fewer than 100 chunks, the MasQA dataset is substantially larger compared to other MDQA datasets, underscoring both the challenges and the practical implications.

![](_page_15_Figure_4.jpeg)

Figure 13: Cohesion within Single Document. (a) The figure illustrates the PCA visualization. (b) The figure depicts the t-SNE visualization.

![](_page_16_Figure_0.jpeg)

Figure 14: Cohesion among Multi-Document.

C.4 How does HCA improve cohesion within homologous sections We visualize all segment vectors from a dataset; then we highlight homologous sections across all documents in this dataset, e.g., all "Application" sections from each manual. As depicted in Figure 15, it is observed that similar segments across different documents become more clustered when processed with HCA, facilitating the answering of cross-document questions.

![](_page_16_Figure_3.jpeg)

Figure 15: Cohesion among Homologous Sections.

C.5 How does HCA improve cohesion in context response We select a Question-Context pair. The question's embedding was marked on the visualization plane. Subsequently, the contexts processed with and without HCA were also plotted to observe their positions and distances relative to the question. As shown in Figure 16, our method significantly reduces the distance between the Context and Question in the embedding space, greatly enhancing retrieval accuracy. This finding corroborates the substantial improvements observed in our method's Log-Rank Index.

![](_page_17_Figure_0.jpeg)

![](_page_17_Figure_1.jpeg)

